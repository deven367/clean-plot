[
  {
    "objectID": "errors.html",
    "href": "errors.html",
    "title": "Errors",
    "section": "",
    "text": "source\n\nMyException\n\n MyException (message)\n\nCommon base class for all non-exit exceptions.\n\ntry:\n    x = float('kjsd')\nexcept MyException('custom error') as e:\n    print(e)\n#     raise MyException(\"cannot divide by zero\")\n\nTypeError: catching classes that do not inherit from BaseException is not allowed",
    "crumbs": [
      "Errors"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Welcome to clean_plot",
    "section": "Install",
    "text": "Install\nThe easiest way to install the library is to simply do a pip install.\npip install clean-plot\nAnother way to install the library would be to build from source. It is more likely that the released version may contain bugs. The source would get updated more often. If you plan to add features to clean_plot yourself, or want to be on the cutting edge, you can use an editable install.\ngit clone https://github.com/deven367/clean_plot.git\ncd clean_plot\npip install -e .",
    "crumbs": [
      "Welcome to clean_plot"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Welcome to clean_plot",
    "section": "How to use",
    "text": "How to use\nThe library contains easy to use methods for cleaning text, tokenizing and lemmatizing sentences. These sentences can then be easily fed to a sentence encoder to create sentence embeddings.\n\nfname = '../files/dummy.txt'\ntext = get_data(fname)\nprint(text)\n\nMARLEY was dead: to begin with. There is no doubt\nwhatever about that. The register of his burial was\nsigned by the clergyman, the clerk, the undertaker,\nand the chief mourner. Scrooge signed it: and\nScrooge's name was good upon 'Change, for anything he\nchose to put his hand to. Old Marley was as dead as a\ndoor-nail.\n\nMind! I don't mean to say that I know, of my\nown knowledge, what there is particularly dead about\na door-nail. I might have been inclined, myself, to\nregard a coffin-nail as the deadest piece of ironmongery\nin the trade. But the wisdom of our ancestors\nis in the simile; and my unhallowed hands\nshall not disturb it, or the Country's done for. You\nwill therefore permit me to repeat, emphatically, that\nMarley was as dead as a door-nail.\n\nThis is a new sentence.\n\n\n\nsentences = make_sentences(text)\n\n\nsentences\n\n(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]\n\n\n\nno_punctuations = []\nfor sentence in sentences:\n    new_sentence = remove_punctuations(sentence)\n    no_punctuations.append(new_sentence)\n\n\nno_punctuations\n\n['MARLEY was dead to begin with',\n 'There is no doubt whatever about that',\n 'The register of his burial was signed by the clergyman the clerk the undertaker and the chief mourner',\n 'Scrooge signed it and Scrooge s name was good upon Change for anything he chose to put his hand to',\n 'Old Marley was as dead as a door nail',\n 'Mind',\n 'I don t mean to say that I know of my own knowledge what there is particularly dead about a door nail',\n 'I might have been inclined myself to regard a coffin nail as the deadest piece of ironmongery in the trade',\n 'But the wisdom of our ancestors is in the simile and my unhallowed hands shall not disturb it or the Country s done for',\n 'You will therefore permit me to repeat emphatically that Marley was as dead as a door nail',\n 'This is a new sentence']",
    "crumbs": [
      "Welcome to clean_plot"
    ]
  },
  {
    "objectID": "index.html#help",
    "href": "index.html#help",
    "title": "Welcome to clean_plot",
    "section": "Help",
    "text": "Help\nTo see the various CLI available in the library, use the function cp_help\n\nchelp()\n\ncheck_len                       Takes name of a txt file and writes the tokenized sentences into a new txt file\ncorr_hm                         Generates correlation plots from normalized SSMs\ncp_help                         Show help for all console scripts\nheatmaps                        Generates plots for embeddings in the folder\nheatmaps_pkl                    Generates SSMs from pkl files\nhistograms                      Generates histograms for embeddings in the folder\nlex_ts                          Generate lexical TS from Lexical SSM\nmake_pkl                        Create pkl for time series from embeddings\nts_pkl                          Plot timeseries from the pkl file",
    "crumbs": [
      "Welcome to clean_plot"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Welcome to clean_plot",
    "section": "Contributing",
    "text": "Contributing\nThis library has come into existence because of nbdev (one of many amazing tools made by fast.ai). PRs and Issues are encouraged.\nAfter you clone this repository, please run nbdev_install_hooks in your terminal. This sets up git hooks, which clean up the notebooks to remove the extraneous stuff stored in the notebooks (e.g. which cells you ran) which causes unnecessary merge conflicts.\nBefore submitting a PR, check that the local library and notebooks match. The script nbdev_fix can let you know if there is a difference between the local library and the notebooks.\nIf you made a change to the notebooks in one of the exported cells, you can export it to the library with nbdev_export.\nIf you made a change to the library, you can export it back to the notebooks with nbdev_update.",
    "crumbs": [
      "Welcome to clean_plot"
    ]
  },
  {
    "objectID": "plot.utils.html",
    "href": "plot.utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\nPlot\n\n Plot (path:str)\n\nPlotting module\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\npath to embeddings\n\n\n\n\nsource\n\n\nPlot.get_normalized\n\n Plot.get_normalized ()\n\nReturns the normalized ssms\n\nsource\n\n\nPlot.get_raw_ssms\n\n Plot.get_raw_ssms ()\n\nReturns the raw ssms\n\nsource\n\n\nPlot.get_standardized\n\n Plot.get_standardized ()\n\nReturns the standardized ssms\n\np = Path('.')\ne = p.absolute().parent.parent/'mnt'/'e'\ncoll_repr(e.ls(), 2)\n\n\nplotter = Plot(e/'A_Modest_Proposal_cleaned')\n\n\nplotter.path.stem.replace('_', ' ')\n\n'A Modest Proposal cleaned'\n\n\n\nplotter\n\nThis object contains the path to `/mnt/e/A_Modest_Proposal_cleaned`\n\n\n\n# d = plotter.get_normalized()\nd = plotter.get_standardized()\n\n\nfor k, v in d.items():\n    print(v.shape)\n\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n(68, 68)\n\n\n\nd.keys()\n\ndict_keys(['DeCLUTR Base', 'DeCLUTR Small', 'DistilBERT', 'InferSent FastText', 'InferSent GloVe', 'MiniLM', 'MPNet', 'RoBERTa', 'USE', 'XLM'])\n\n\n\nplotter.get_sectional_ssms(0, -1, True)\n\nDone plotting A Modest Proposal DeCLUTR Base.png\nDone plotting A Modest Proposal DeCLUTR Small.png\nDone plotting A Modest Proposal DistilBERT.png\nDone plotting A Modest Proposal InferSent FastText.png\nDone plotting A Modest Proposal InferSent GloVe.png\nDone plotting A Modest Proposal MiniLM.png\nDone plotting A Modest Proposal MPNet.png\nDone plotting A Modest Proposal RoBERTa.png\nDone plotting A Modest Proposal USE.png\nDone plotting A Modest Proposal XLM.png\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nplotter.create_ssms()\n\nDone plotting A Modest Proposal DeCLUTR Small.png\nDone plotting A Modest Proposal RoBERTa.png\nDone plotting A Modest Proposal InferSent GloVe.png\nDone plotting A Modest Proposal InferSent FastText.png\nDone plotting A Modest Proposal DistilBERT.png\nDone plotting A Modest Proposal MPNet.png\nDone plotting A Modest Proposal USE.png\nDone plotting A Modest Proposal DeCLUTR Base.png\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nclass Foo:\n    def __init__(self, i):\n        self.i = i\n    def __repr__(self):\n        return f'Init value in i is {self.i}'\n    def __str__(self):\n        return f'idk what this does'\n\n\nx = Foo(5)\n\n\nx\n\nInit value in i is 5\n\n\n\nprint(x)\n\nidk what this does",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "pickle.html",
    "href": "pickle.html",
    "title": "Exporting to pickle",
    "section": "",
    "text": "NOTE The module has 2 main functions: * create_dic which creates individual .pkl files (for each chapter of the book) based on breakpoints of chapters given by the use. (use when the dataset is very huge. Visualizing the entire heatmap does not give a lot of information. * create_dic_whole_book creates a single .pkl file for the entire book. To be used when the dataset is relatively small in size i.e. 2000 - 2500 sentences.\n\nsource\n\nlabel\n\n label (method:str)\n\nReturns the full name of the model based on the abbreviation\n\n\n\n\nType\nDetails\n\n\n\n\nmethod\nstr\nname of the method\n\n\n\n\nsource\n\n\ncos_sim\n\n cos_sim (a:numpy.ndarray, b:numpy.ndarray)\n\nReturns the cosine similarity between 2 vectors.\n\n\n\n\nType\nDetails\n\n\n\n\na\nnp.ndarray\nvector 1\n\n\nb\nnp.ndarray\nvector 2\n\n\n\n\nsource\n\n\nsuccessive_similarities\n\n successive_similarities (embeddings, k)\n\n\nsource\n\n\ncreate_dict_whole_book\n\n create_dict_whole_book (embedding_path:str='.', k:int=1)\n\nCreate pkl for time series from embeddings\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nembedding_path\nstr\n.\npath to the embeddings\n\n\nk\nint\n1\nconsecutive index\n\n\n\n\nsource\n\n\ncreate_label_whole_book\n\n create_label_whole_book (method, parent_dir)\n\n\nsource\n\n\ncreate_label\n\n create_label (index, method, parent_dir)\n\n\nsource\n\n\nget_embed_method_and_name\n\n get_embed_method_and_name (fname)\n\nReturns the name of the file and the method by splitting on the word ‘cleaned’\n\n\n\n\nType\nDetails\n\n\n\n\nfname\n\nname of the file\n\n\nReturns\n(str, str)\nname of file, embeddding method\n\n\n\n\n\n\n/home/deven\n\n\n\ncreate_dict_whole_book('embeddings/A_Modest_Proposal', 1)\n\nBook Name: A Modest Proposal\nFound 10 methods\n---------------------------------------------\nFound DeCLUTR Small\nFound RoBERTa\nFound InferSent GloVe\nFound InferSent FastText\nFound DistilBERT\nFound XLM\nFound MPNet\nFound USE\nFound DeCLUTR Base\nFound MiniLM\n---------------------------------------------\nSaved pkl at /home/deven/embeddings/A_Modest_Proposal/pkl",
    "crumbs": [
      "Exporting to pickle"
    ]
  },
  {
    "objectID": "tutorials/setup.html",
    "href": "tutorials/setup.html",
    "title": "Setup",
    "section": "",
    "text": "def foo():\n    \"this function does nothing\"\n    return\n\n\nx = 5\ny = x/0\n\nZeroDivisionError: division by zero",
    "crumbs": [
      "tutorials",
      "Setup"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n loader (path:str|Path, extension:str, recursive:bool=True,\n         symlinks:bool=True, file_glob:str=None, file_re:str=None,\n         folder_re:str=None, skip_file_glob:str=None,\n         skip_file_re:str=None, skip_folder_re:str=None,\n         func:callable=&lt;function join&gt;, ret_folders:bool=False)\n\nGiven a Path and an extension, returns all files with the extension in the path\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n\npath to a given folder,\n\n\nextension\nstr\n\nextension of the file you want\n\n\nrecursive\nbool\nTrue\nsearch subfolders\n\n\nsymlinks\nbool\nTrue\nfollow symlinks?\n\n\nfile_glob\nstr\nNone\nOnly include files matching glob\n\n\nfile_re\nstr\nNone\nOnly include files matching regex\n\n\nfolder_re\nstr\nNone\nOnly enter folders matching regex\n\n\nskip_file_glob\nstr\nNone\nSkip files matching glob\n\n\nskip_file_re\nstr\nNone\nSkip files matching regex\n\n\nskip_folder_re\nstr\nNone\nSkip folders matching regex,\n\n\nfunc\ncallable\njoin\nfunction to apply to each matched file\n\n\nret_folders\nbool\nFalse\nreturn folders, not just files\n\n\nReturns\nL\n\nreturns L\n\n\n\n\nsource\n\n\n\n\n get_data (fname:str|pathlib.Path)\n\nReads from a txt file\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr | Path\npath to the file\n\n\nReturns\nstr\nreturns content of the file\n\n\n\n\nsource\n\n\n\n\n load_pmi (fname:str|pathlib.Path)\n\nLoads the PMI matrix\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr | Path\n\n\n\nReturns\nnp.ndarray\nname of pmi file # pmi matrix\n\n\n\n\nx = np.random.randint(0 , 100, (100, 100))\nnp.save('test.npy', x)\nread_file = load_pmi('test.npy')\n\nLoaded test.npy\n\n\n\ntest_eq(x, read_file)\n\n\nsource\n\n\n\n\n load_dictionary (fname:str)\n\nGiven a fname, function loads a pkl dictionary from the current directory\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr\npath to the pkl file\n\n\nReturns\ndict\nreturns the contents\n\n\n\n\nsource\n\n\n\n\n normalize (data:numpy.ndarray)\n\nGiven an input array, return normalized array\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nnp.ndarray\ninput array\n\n\nReturns\nnp.ndarray\nnormalized array\n\n\n\n\ntest_eq(normalize([1, 2, 3, 4, 5]), [0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\nsource\n\n\n\n\n chelp ()\n\nShow help for all console scripts\n\nchelp()\n\nclean_file                Takes name of a txt file and writes the tokenized sentences into a new txt file\ncorr_hm                   Generates correlation plots from normalized SSMs\ncp_help                   Show help for all console scripts\nheatmaps                  Generates plots for embeddings in the folder\nheatmaps_pkl              Generates SSMs from pkl files\nhistograms                Generates histograms for embeddings in the folder\nlex_ts                    Generate lexical TS from Lexical SSM\nmake_pkl                  Create pkl for time series from embeddings\nts_pkl                    Plot timeseries from the pkl file",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#generic-utils",
    "href": "utils.html#generic-utils",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n loader (path:str|Path, extension:str, recursive:bool=True,\n         symlinks:bool=True, file_glob:str=None, file_re:str=None,\n         folder_re:str=None, skip_file_glob:str=None,\n         skip_file_re:str=None, skip_folder_re:str=None,\n         func:callable=&lt;function join&gt;, ret_folders:bool=False)\n\nGiven a Path and an extension, returns all files with the extension in the path\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | Path\n\npath to a given folder,\n\n\nextension\nstr\n\nextension of the file you want\n\n\nrecursive\nbool\nTrue\nsearch subfolders\n\n\nsymlinks\nbool\nTrue\nfollow symlinks?\n\n\nfile_glob\nstr\nNone\nOnly include files matching glob\n\n\nfile_re\nstr\nNone\nOnly include files matching regex\n\n\nfolder_re\nstr\nNone\nOnly enter folders matching regex\n\n\nskip_file_glob\nstr\nNone\nSkip files matching glob\n\n\nskip_file_re\nstr\nNone\nSkip files matching regex\n\n\nskip_folder_re\nstr\nNone\nSkip folders matching regex,\n\n\nfunc\ncallable\njoin\nfunction to apply to each matched file\n\n\nret_folders\nbool\nFalse\nreturn folders, not just files\n\n\nReturns\nL\n\nreturns L\n\n\n\n\nsource\n\n\n\n\n get_data (fname:str|pathlib.Path)\n\nReads from a txt file\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr | Path\npath to the file\n\n\nReturns\nstr\nreturns content of the file\n\n\n\n\nsource\n\n\n\n\n load_pmi (fname:str|pathlib.Path)\n\nLoads the PMI matrix\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr | Path\n\n\n\nReturns\nnp.ndarray\nname of pmi file # pmi matrix\n\n\n\n\nx = np.random.randint(0 , 100, (100, 100))\nnp.save('test.npy', x)\nread_file = load_pmi('test.npy')\n\nLoaded test.npy\n\n\n\ntest_eq(x, read_file)\n\n\nsource\n\n\n\n\n load_dictionary (fname:str)\n\nGiven a fname, function loads a pkl dictionary from the current directory\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr\npath to the pkl file\n\n\nReturns\ndict\nreturns the contents\n\n\n\n\nsource\n\n\n\n\n normalize (data:numpy.ndarray)\n\nGiven an input array, return normalized array\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nnp.ndarray\ninput array\n\n\nReturns\nnp.ndarray\nnormalized array\n\n\n\n\ntest_eq(normalize([1, 2, 3, 4, 5]), [0.  , 0.25, 0.5 , 0.75, 1.  ])\n\n\nsource\n\n\n\n\n chelp ()\n\nShow help for all console scripts\n\nchelp()\n\nclean_file                Takes name of a txt file and writes the tokenized sentences into a new txt file\ncorr_hm                   Generates correlation plots from normalized SSMs\ncp_help                   Show help for all console scripts\nheatmaps                  Generates plots for embeddings in the folder\nheatmaps_pkl              Generates SSMs from pkl files\nhistograms                Generates histograms for embeddings in the folder\nlex_ts                    Generate lexical TS from Lexical SSM\nmake_pkl                  Create pkl for time series from embeddings\nts_pkl                    Plot timeseries from the pkl file",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#utils-for-cleaning-text",
    "href": "utils.html#utils-for-cleaning-text",
    "title": "Utils",
    "section": "Utils for cleaning text",
    "text": "Utils for cleaning text\nBefore using any of the cleaning utils in the file, please run download_nltk_dep first.\n\nsource\n\ndownload_nltk_dep\n\n download_nltk_dep ()\n\nDownloads the nltk dependencies\n\nsource\n\n\nsplit_by_newline\n\n split_by_newline (text:str)\n\nOnly use when sentences are already tokenized returns sentences split by \n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nsentences separated by \n\n\nReturns\nL\nlist of sentences\n\n\n\n\ntext = \"Hello there!\\nThis is how this functions works!\"\nsplit_by_newline(text)\n\n(#2) ['Hello there!','This is how this functions works!']\n\n\n\nsource\n\n\nrm_useless_spaces\n\n rm_useless_spaces (t:str)\n\nRemoves useless spaces\n\n\n\n\nType\nDetails\n\n\n\n\nt\nstr\nsentence with extra spaces\n\n\nReturns\nstr\nsentence without extra spaces\n\n\n\n\nrm_useless_spaces('  This is      test sentence.  This removes  all the extra  spaces.  ')\n\n'This is test sentence. This removes all the extra spaces.'\n\n\n\nsource\n\n\nmake_sentences\n\n make_sentences (text:str)\n\nConverts given bulk into sentences\n\n\n\n\nType\nDetails\n\n\n\n\ntext\nstr\nbulk text\n\n\nReturns\nL\nlist of sentences\n\n\n\n\nsource\n\n\nwrite_to_file_cleaned\n\n write_to_file_cleaned (sentences:list, fname:str)\n\nWrites the sentences to a .txt file\n\n\n\n\nType\nDetails\n\n\n\n\nsentences\nlist\nlist of sentences\n\n\nfname\nstr\nname of output file\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nclean\n\n clean (fname:str)\n\nTakes name of a txt file and writes the tokenized sentences into a new txt file\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr\nname of input txt file\n\n\nReturns\nNone\n\n\n\n\nAll functions mentioned above are merged into a single function called clean. You only need to give it the name of the .txt file that you want to clean and call the function\n\nfname = '../files/dummy.txt'\ntext = get_data(fname)\nprint(text)\n\nMARLEY was dead: to begin with. There is no doubt\nwhatever about that. The register of his burial was\nsigned by the clergyman, the clerk, the undertaker,\nand the chief mourner. Scrooge signed it: and\nScrooge's name was good upon 'Change, for anything he\nchose to put his hand to. Old Marley was as dead as a\ndoor-nail.\n\nMind! I don't mean to say that I know, of my\nown knowledge, what there is particularly dead about\na door-nail. I might have been inclined, myself, to\nregard a coffin-nail as the deadest piece of ironmongery\nin the trade. But the wisdom of our ancestors\nis in the simile; and my unhallowed hands\nshall not disturb it, or the Country's done for. You\nwill therefore permit me to repeat, emphatically, that\nMarley was as dead as a door-nail.\n\nThis is a new sentence.\n\n\nIt goes from this to\n\nmake_sentences(get_data(fname))\n\n(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]\n\n\nThe clean function writes these sentences into a txt file with the name &lt;fname&gt;_cleaned.txt\n\nsource\n\n\nget_wordnet_pos\n\n get_wordnet_pos (word:str)\n\nMap POS tag to first character lemmatize() accepts\n\n\n\n\nType\nDetails\n\n\n\n\nword\nstr\ninput word token\n\n\nReturns\nstr\nPOS of the given word\n\n\n\n\nsource\n\n\nremove_stopwords\n\n remove_stopwords (sentence:str)\n\nTakes a sentence and removes stopwords from it\n\n\n\n\nType\nDetails\n\n\n\n\nsentence\nstr\ninput sentence\n\n\nReturns\nstr\noutput sentence\n\n\n\n\nsource\n\n\nremove_punctuations\n\n remove_punctuations (sentence:str)\n\nTakes a sentence and removes punctuations from it\n\n\n\n\nType\nDetails\n\n\n\n\nsentence\nstr\ninput sentence\n\n\nReturns\nstr\noutput sentence\n\n\n\n\nsource\n\n\nremove_punc_clean\n\n remove_punc_clean (sentence:str, lemmatize:bool=False)\n\n*Takes a sentence and removes punctuations and stopwords from it\nWill lemmatize words if lemmatize = True*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsentence\nstr\n\ninput sentence\n\n\nlemmatize\nbool\nFalse\nflag to lemmatize\n\n\nReturns\nstr\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible that while using remove_punc_clean, a sentence might get eliminated completely as it only contained stopwords.\n\n\n\nsource\n\n\nprocess_for_lexical\n\n process_for_lexical (fname:str)\n\nGiven an input txt file, return removed sentences\n\n\n\n\nType\nDetails\n\n\n\n\nfname\nstr\nname of the input txt file\n\n\nReturns\nL\n\n\n\n\n\n\nExample contd.\n\ndata = get_data(fname)\nsentences = make_sentences(data)\nsentences\n\n(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]\n\n\nLet’s continue the same example from above\nHere, the remove_punc_clean function removes punctuations, STOPWORDS and lemmatizes the word and returns the cleaned sentence.\n\nIt is possible that a sentence may be removed completely as it may contain only STOPWORDS.\n\nThis method is to be used for methods involving lexical analysis.\nWithout lemmatization\n\nfor sentence in sentences:\n    print(remove_punc_clean(sentence))\n\nMARLEY dead begin\ndoubt whatever\nregister burial signed clergyman clerk undertaker chief mourner\nScrooge signed Scrooge name good upon Change anything chose put hand\nOld Marley dead door nail\nMind\nmean say know knowledge particularly dead door nail\nmight inclined regard coffin nail deadest piece ironmongery trade\nwisdom ancestors simile unhallowed hands shall disturb Country done\ntherefore permit repeat emphatically Marley dead door nail\nnew sentence\n\n\nWith Lemmatization\n\nfor sentence in sentences:\n    print(remove_punc_clean(sentence, lemmatize=True))\n\nMARLEY dead begin\ndoubt whatever\nregister burial sign clergyman clerk undertaker chief mourner\nScrooge sign Scrooge name good upon Change anything chose put hand\nOld Marley dead door nail\nMind\nmean say know knowledge particularly dead door nail\nmight inclined regard coffin nail deadest piece ironmongery trade\nwisdom ancestor simile unhallowed hand shall disturb Country do\ntherefore permit repeat emphatically Marley dead door nail\nnew sentence\n\n\n\nclean('../files/dummy.txt')\n\ndummy.txt contains 11 sentences\n\n\n\nprocess_for_lexical('../files/dummy.txt')\n\nDone processing dummy.txt\n\n\n(#0) []\n\n\n\nPath('../files/').ls()\n\n(#2) [Path('../files/dummy.txt'),Path('../files/dummy_cleaned.txt')]\n\n\n\nsource\n\n\nnum_words\n\n num_words (sentence:str)\n\nReturns the number of words in a sentence\n\n\n\n\nType\nDetails\n\n\n\n\nsentence\nstr\ninput sentence\n\n\nReturns\nint\nnumber of words\n\n\n\n\nprint(sentences[0])\nnum_words(sentences[0])\n\nMARLEY was dead: to begin with.\n\n\n6\n\n\n\nprint(sentences[1])\nnum_words(sentences[1])\n\nThere is no doubt whatever about that.\n\n\n7\n\n\n\n\nPatches to pathlib.Path\nWith all these utility functions, these are just some additional functions which are applied to pathlib.Path. There are 3 additional functions/properties when you have a numpy array or a txt file inside a Path object.\n\nsource\n\n\nPath.shape\n\n Path.shape ()\n\nImagine I read a numpy array and I wish to see its shape. If I were to use the regular route, I would have to…\n\nwith working_directory('/home/deven'):\n    p = 'test.npy'\n    arr = np.load(p)\narr.shape\n\n(100, 100)\n\n\nInstead of all of that, I can just call Path().shape, like this\n\nwith working_directory('/home/deven'):\n    shp = Path('test.npy').shape\n    test_eq(arr.shape, Path('test.npy').shape)\n\n\nsource\n\n\nPath.text\n\n Path.text ()\n\nUsing this same logic, when I have a txt file inside a Path object\n\nPath('../files/dummy.txt').text\n\n\"MARLEY was dead: to begin with. There is no doubt\\nwhatever about that. The register of his burial was\\nsigned by the clergyman, the clerk, the undertaker,\\nand the chief mourner. Scrooge signed it: and\\nScrooge's name was good upon 'Change, for anything he\\nchose to put his hand to. Old Marley was as dead as a\\ndoor-nail.\\n\\nMind! I don't mean to say that I know, of my\\nown knowledge, what there is particularly dead about\\na door-nail. I might have been inclined, myself, to\\nregard a coffin-nail as the deadest piece of ironmongery\\nin the trade. But the wisdom of our ancestors\\nis in the simile; and my unhallowed hands\\nshall not disturb it, or the Country's done for. You\\nwill therefore permit me to repeat, emphatically, that\\nMarley was as dead as a door-nail.\\n\\nThis is a new sentence.\"\n\n\n\nsource\n\n\nPath.sentences\n\n Path.sentences ()\n\n\nPath('../files/dummy.txt').sentences\n\n(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]",
    "crumbs": [
      "Utils"
    ]
  },
  {
    "objectID": "heatmaps_novels.html",
    "href": "heatmaps_novels.html",
    "title": "Heatmaps",
    "section": "",
    "text": "source\n\n\n\n heatmap_from_pkl (path:str='.', min_labels:bool=False, std:bool=False,\n                   corr:bool=False)\n\nPlot timeseries from the pkl file\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n.\npath to pkl files\n\n\nmin_labels\nbool\nFalse\nflag to use shorter labels\n\n\nstd\nbool\nFalse\nflag to standardize\n\n\ncorr\nbool\nFalse\nflag to save corr plot\n\n\nReturns\nNone\n\n\n\n\n\n\nheatmap_from_pkl(path = 'embeddings/A_Modest_Proposal/pkl', \n                min_labels=True,\n                std = True,\n                corr=True)\n\nCurrent path /home/deven/embeddings/A_Modest_Proposal/pkl\nFound 1 pkl files\n---------------------------------------------\nA Modest Proposal\n---------------------------------------------\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;",
    "crumbs": [
      "Heatmaps"
    ]
  },
  {
    "objectID": "heatmaps_novels.html#methods-of-pkl-files",
    "href": "heatmaps_novels.html#methods-of-pkl-files",
    "title": "Heatmaps",
    "section": "",
    "text": "source\n\n\n\n heatmap_from_pkl (path:str='.', min_labels:bool=False, std:bool=False,\n                   corr:bool=False)\n\nPlot timeseries from the pkl file\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n.\npath to pkl files\n\n\nmin_labels\nbool\nFalse\nflag to use shorter labels\n\n\nstd\nbool\nFalse\nflag to standardize\n\n\ncorr\nbool\nFalse\nflag to save corr plot\n\n\nReturns\nNone\n\n\n\n\n\n\nheatmap_from_pkl(path = 'embeddings/A_Modest_Proposal/pkl', \n                min_labels=True,\n                std = True,\n                corr=True)\n\nCurrent path /home/deven/embeddings/A_Modest_Proposal/pkl\nFound 1 pkl files\n---------------------------------------------\nA Modest Proposal\n---------------------------------------------\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;",
    "crumbs": [
      "Heatmaps"
    ]
  },
  {
    "objectID": "heatmaps_novels.html#methods-of-ssms",
    "href": "heatmaps_novels.html#methods-of-ssms",
    "title": "Heatmaps",
    "section": "Methods of SSMs",
    "text": "Methods of SSMs\n\nsource\n\nplot_novels\n\n plot_novels (path:str=None, start:int=0, end:int=-1, x:bool=False,\n              y:int=5, std:bool=False)\n\nGenerates plots for embeddings in the folder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\nNone\npath for embeddings\n\n\nstart\nint\n0\nstart for section\n\n\nend\nint\n-1\nend for section\n\n\nx\nbool\nFalse\nx-ticks\n\n\ny\nint\n5\ny-ticks,\n\n\nstd\nbool\nFalse\nflag to standardize\n\n\n\n\nplot_novels('embeddings/A_Modest_Proposal/', start=10, end=15)\n\nFound 10 npy files\n---------------------------------------------\nDone plotting A Modest Proposal DeCLUTR Small.png\nDone plotting A Modest Proposal RoBERTa.png\nDone plotting A Modest Proposal InferSent GloVe.png\nDone plotting A Modest Proposal InferSent FastText.png\nDone plotting A Modest Proposal DistilBERT.png\nDone plotting A Modest Proposal XLM.png\nDone plotting A Modest Proposal MPNet.png\nDone plotting A Modest Proposal USE.png\nDone plotting A Modest Proposal DeCLUTR Base.png\nDone plotting A Modest Proposal MiniLM.png\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nsource\n\n\nplot_histograms\n\n plot_histograms (path:str, std:bool=False)\n\nGenerates histograms for embeddings in the folder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath for embeddings\n\n\nstd\nbool\nFalse\nflag to standardize\n\n\n\n\nsource\n\n\nssms_from_pkl\n\n ssms_from_pkl (path:str, start:int=0, end:int=-1, x:bool=False, y:int=5)\n\nGenerates SSMs from pkl files\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath for pkl file\n\n\nstart\nint\n0\nstart for section\n\n\nend\nint\n-1\nend for section\n\n\nx\nbool\nFalse\nx-ticks\n\n\ny\nint\n5\ny-ticks\n\n\n\n\nsource\n\n\ncorr_heatmaps\n\n corr_heatmaps (path:str, std:bool=False)\n\nGenerates correlation plots from normalized SSMs\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath for embeddings\n\n\nstd\nbool\nFalse\nstandardize or not\n\n\n\n\nsource\n\n\ncorr_ts\n\n corr_ts (path:str)\n\nGenerates correlation plots from time series\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\npath for embeddings\n\n\n\n\nsource\n\n\nlex_ts\n\n lex_ts (path:str)\n\nGenerate lexical TS from Lexical SSM\n\n\n\n\nType\nDetails\n\n\n\n\npath\nstr\npath for embeddings\n\n\n\n\nsource\n\n\nplot_standardized\n\n plot_standardized (path:str, start:int=0, end:int=-1, x:bool=False,\n                    y:int=5)\n\nGenerates plots for embeddings in the folder\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\npath for embeddings\n\n\nstart\nint\n0\nstart for section\n\n\nend\nint\n-1\nend for section\n\n\nx\nbool\nFalse\nx-ticks\n\n\ny\nint\n5\ny-ticks",
    "crumbs": [
      "Heatmaps"
    ]
  },
  {
    "objectID": "lexical.html",
    "href": "lexical.html",
    "title": "Lexical",
    "section": "",
    "text": "This module will be rewritten. Use at your own risk!",
    "crumbs": [
      "Lexical"
    ]
  }
]