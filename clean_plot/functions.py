# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_functions.ipynb.

# %% auto 0
__all__ = ['download_ntlk_dep', 'normalize', 'split_by_newline', 'rm_useless_spaces', 'make_sentences', 'write_to_file_cleaned',
           'clean', 'get_wordnet_pos', 'remove_stopwords', 'remove_punctuations', 'remove_punc_clean', 'process']

# %% ../nbs/01_functions.ipynb 4
from .core import *
from pathlib import Path
import os
import pandas as pd
import pickle
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import unidecode
import re

from fastcore.foundation import L
from fastcore.test import test_eq, test_ne
from fastcore.script import call_parse

# %% ../nbs/01_functions.ipynb 5
from typing import Callable, Iterator, Union, Optional, List

# %% ../nbs/01_functions.ipynb 6
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import wordnet, stopwords
from nltk.stem import WordNetLemmatizer

# %% ../nbs/01_functions.ipynb 8
def download_ntlk_dep():
    """
    Downloads the `nltk` dependencies
    """
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('wordnet')
    nltk.download('omw-1.4')

# %% ../nbs/01_functions.ipynb 9
def normalize(data: np.ndarray) -> np.ndarray:
    """
    The function takes an array, matrix as input and normalizes
    it between 0 and 1

    Args:
        data (ndarray): any 1-D, or 2-D numpy array

    Returns:
        (ndarray): normalized ndarray
    """
    return (data - np.min(data)) / (np.max(data) - np.min(data))

# %% ../nbs/01_functions.ipynb 12
def split_by_newline(text: str) -> List[str]:
    """
    Only use when sentences are already tokenized
    returns sentences split by '\\n' if len(line) > 0

    Args:
        all (str): tokenized string to be split by '\\n'

    Returns:
        list: list of sentences split by '\\n'
    """
    return L([line for line in text.split('\n') if len(line) > 0])

# %% ../nbs/01_functions.ipynb 14
def rm_useless_spaces(t: str) -> str: 
    """
    Removes useless spaces
    """
    _re_space = re.compile(' {2,}')
    return _re_space.sub(' ', t).lstrip().rstrip()

# %% ../nbs/01_functions.ipynb 16
def make_sentences(text: str) -> List[str]:
    """
    Converts given bulk into sentences
    """
    try:
        sent_tokenize('')
    except Exception as error:
        download_ntlk_dep()
        print(f'Run download_nltk_dep() first') 
#     all_cleaned = re.sub('\n', ' ', text)
    all_cleaned = text.replace('\n', ' ')
    all_cleaned = rm_useless_spaces(all_cleaned)
    all_cleaned = all_cleaned.strip()
    all_cleaned = unidecode.unidecode(all_cleaned)
    sentences = sent_tokenize(all_cleaned)
    return L(sentences)

# %% ../nbs/01_functions.ipynb 17
def write_to_file_cleaned(sentences: List[str], fname: str) -> None:
    """
    Writes the sentences to a .txt file
    """
    with open(f'{fname.stem}_cleaned.txt', 'w') as f:
        for line in sentences:
            f.write(f'{line}\n')
    f.close()

# %% ../nbs/01_functions.ipynb 18
@call_parse
def clean(fname: str) -> None:
    """
    Takes name of a txt file and writes the tokenized sentences into a new txt file
    """
    fname = Path(fname)
    text = get_data(fname)
    sentences = make_sentences(text)
    print(f'{fname.name} contains {len(sentences)} sentences')
    write_to_file_cleaned(sentences, fname)

# %% ../nbs/01_functions.ipynb 25
def get_wordnet_pos(word: str) -> str:
    """Map POS tag to first character lemmatize() accepts"""
    try:
        nltk.pos_tag('x')
    except:
        print(f'Run download_nltk_dep() first')
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

# %% ../nbs/01_functions.ipynb 26
def remove_stopwords(sentence: str) -> str:
    """
    Takes a sentence and removes stopwords from it
    """
    try:
        stopwords.words('english')
    except:
        print(f'Run download_nltk_dep() first')
    sentences = []
    STOPWORDS = set(stopwords.words('english'))
    for word in sentence.split():
        if word.lower() not in STOPWORDS:
            sentences.append(word)
    return ' '.join(sentences)

# %% ../nbs/01_functions.ipynb 27
def remove_punctuations(sentence: str) -> str:
    """
    Takes a sentence and removes punctuations from it
    """
    pat2 = re.compile('[^a-zA-Z0-9 ]+')
    pat1 = re.compile('[\s]+')

    doc = pat2.sub(' ', sentence)
    doc = pat1.sub(' ', doc)
    doc = doc.strip()
    return doc

# %% ../nbs/01_functions.ipynb 28
def remove_punc_clean(sentence: str, lemmatize: bool = False) -> str:
    """
    Takes a sentence and removes punctuations and stopwords from it
    
    Will lemmatize words if `lemmatize = True`
    """
    doc = remove_punctuations(sentence)
    doc = remove_stopwords(doc)
    
    
    if lemmatize:
        lemmatizer = WordNetLemmatizer()
        doc = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc.split()])
    return doc

# %% ../nbs/01_functions.ipynb 29
def process(fname: str) -> List[str]:
    fname = Path(fname)
    all_data = get_data(fname)
    sentences = make_sentences(all_data)
    clean_sentences = []
    removed_sentences = []
    for i, sentence in enumerate(sentences):
        t = remove_punc_clean(sentence)
        if len(t) > 0:
            clean_sentences.append(t)
        else:
            removed_sentences.append(i)

    # write_to_file_lexical(clean_sentences, fname)
    print('Done processing', fname.name)
    return L(removed_sentences)
