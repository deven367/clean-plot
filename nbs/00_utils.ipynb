{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c03cfe",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Various utils for cleaning, organizing and capturing other information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326490e4",
   "metadata": {},
   "source": [
    "## Generic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from fastcore.foundation import L, patch\n",
    "from fastcore.meta import delegates\n",
    "from fastcore.script import call_parse\n",
    "from fastcore.xtras import globtastic\n",
    "import nltk\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deven\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "#| local\n",
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def check_files(files):  # files to validate\n",
    "    flen = len(files)\n",
    "    if flen <= 0:\n",
    "        print(f\"Found {flen} files\")\n",
    "        print(\"Check `path` and try again\")\n",
    "        return False\n",
    "\n",
    "    if isinstance(files[0], str):\n",
    "        _type = \"npy\" if files[0].endswith(\"npy\") else \"pkl\"\n",
    "    elif isinstance(files[0], Path):\n",
    "        _type = \"npy\" if files[0].name.endswith(\"npy\") else \"pkl\"\n",
    "\n",
    "    print(f\"Found {flen} {_type} files\")\n",
    "    print(\"-\" * 45)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aaf914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files\n",
      "Check `path` and try again\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#| local\n",
    "files = globtastic('embeddings/A_Modest_Proposal/pkl', recursive=False)\n",
    "_ = check_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd6db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files\n",
      "Check `path` and try again\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#| local\n",
    "files = globtastic('embeddings/A_Modest_Proposal/', recursive=False)\n",
    "_ = check_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fba8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(globtastic)\n",
    "def loader(\n",
    "    path: str | Path,  # path to a given folder,\n",
    "    extension: str,  # extension of the file you want\n",
    "    **kwargs,\n",
    ") -> L:  # returns `L`\n",
    "    \"\"\"Given a Path and an extension, returns all files with the extension\n",
    "    in the path\"\"\"\n",
    "    files = globtastic(path, file_glob=f\"*{extension}\", **kwargs).map(Path)\n",
    "\n",
    "    return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_data(\n",
    "    fname: str | Path,  # path to the file\n",
    ") -> str:  # returns content of the file\n",
    "    \"Reads from a txt file\"\n",
    "    with open(fname, \"r\") as f:\n",
    "        all_text = f.read()\n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73089b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_pmi(fname: str | Path) -> np.ndarray:  # name of pmi file matrix\n",
    "    \"\"\"\n",
    "    Loads the PMI matrix\n",
    "    \"\"\"\n",
    "    file_ = loader(fname, \".npy\")\n",
    "    pmi = np.load(file_[0])\n",
    "    print(f\"Loaded {fname}\")\n",
    "    return pmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1188aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test.npy\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(0 , 100, (100, 100))\n",
    "np.save('test.npy', x)\n",
    "read_file = load_pmi('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ace0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(x, read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16658b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_dictionary(\n",
    "    fname: str,  # path to the pkl file\n",
    ") -> dict:  # returns the contents\n",
    "    \"\"\"\n",
    "    Given a fname, function loads a `pkl` dictionary\n",
    "    from the current directory\n",
    "    \"\"\"\n",
    "    fname = open(fname, \"rb\")\n",
    "    data = pickle.load(fname)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def normalize(\n",
    "    data: np.ndarray,  # input array\n",
    ") -> np.ndarray:  # normalized array\n",
    "    \"\"\"\n",
    "    Given an input array, return normalized array\n",
    "    \"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd14c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(normalize([1, 2, 3, 4, 5]), [0.  , 0.25, 0.5 , 0.75, 1.  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@call_parse\n",
    "def chelp():\n",
    "    \"Show help for all console scripts\"\n",
    "    from fastcore.xtras import console_help\n",
    "\n",
    "    console_help(\"clean_plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94mclean_file\u001b[0m                      Takes name of a txt file and writes the tokenized sentences into a\n",
      "    new txt file\n",
      "\u001b[1m\u001b[94mcorr_hm\u001b[0m                         Generates correlation plots from normalized SSMs\n",
      "\u001b[1m\u001b[94mcp_help\u001b[0m                         Show help for all console scripts\n",
      "\u001b[1m\u001b[94mheatmaps\u001b[0m                        Generates plots for embeddings in the folder\n",
      "\u001b[1m\u001b[94mheatmaps_pkl\u001b[0m                    Generates SSMs from pkl files\n",
      "\u001b[1m\u001b[94mhistograms\u001b[0m                      Generates histograms for embeddings in the folder\n",
      "\u001b[1m\u001b[94mlex_ts\u001b[0m                          Generate lexical TS from Lexical SSM\n",
      "\u001b[1m\u001b[94mmake_pkl\u001b[0m                        Create pkl for time series from embeddings\n",
      "\u001b[1m\u001b[94mts_pkl\u001b[0m                          Plot timeseries from the pkl file\n"
     ]
    }
   ],
   "source": [
    "chelp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcbf58",
   "metadata": {},
   "source": [
    "## Utils for cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1571a87",
   "metadata": {},
   "source": [
    "Before using any of the cleaning utils in the file, please run `download_nltk_dep` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f592c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def download_nltk_dep():\n",
    "    \"\"\"\n",
    "    Downloads the `nltk` dependencies\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"averaged_perceptron_tagger\")\n",
    "    nltk.download(\"wordnet\")\n",
    "    nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c07c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dependencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/deven/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/deven/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/deven/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/deven/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/deven/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except:\n",
    "    print('Downloading dependencies')\n",
    "    download_nltk_dep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ad0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def split_by_newline(\n",
    "    text: str,  # sentences separated by \\n\n",
    ") -> L:  # list of sentences\n",
    "    \"\"\"\n",
    "    Only use when sentences are already tokenized\n",
    "    returns sentences split by `\\n`\n",
    "    \"\"\"\n",
    "    return L([line for line in text.split(\"\\n\") if len(line) > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8a7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) ['Hello there!','This is how this functions works!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello there!\\nThis is how this functions works!\"\n",
    "split_by_newline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rm_useless_spaces(\n",
    "    t: str,  # sentence with extra spaces\n",
    ") -> str:  # sentence without extra spaces\n",
    "    \"\"\"\n",
    "    Removes useless spaces\n",
    "    \"\"\"\n",
    "    _re_space = re.compile(\" {2,}\")\n",
    "    return _re_space.sub(\" \", t).lstrip().rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdb50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is test sentence. This removes all the extra spaces.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_useless_spaces('  This is      test sentence.  This removes  all the extra  spaces.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def make_sentences(text: str) -> L:  # bulk text  # list of sentences\n",
    "    \"\"\"\n",
    "    Converts given bulk into sentences\n",
    "    \"\"\"\n",
    "    all_cleaned = text.replace(\"\\n\", \" \")\n",
    "    all_cleaned = rm_useless_spaces(all_cleaned)\n",
    "    all_cleaned = all_cleaned.strip()\n",
    "    all_cleaned = unidecode.unidecode(all_cleaned)\n",
    "    sentences = sent_tokenize(all_cleaned)\n",
    "    return L(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71078cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def write_to_file_cleaned(\n",
    "    sentences: list, fname: str  # list of sentences  # name of output file\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Writes the sentences to a .txt file\n",
    "    \"\"\"\n",
    "    with open(f\"{fname.stem}_cleaned.txt\", \"w\") as f:\n",
    "        for line in sentences:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@call_parse\n",
    "def clean(fname: str) -> None:  # name of input txt file\n",
    "    \"\"\"Takes name of a txt file and writes the tokenized sentences into a\n",
    "    new txt file\"\"\"\n",
    "    fname = Path(fname)\n",
    "    text = get_data(fname)\n",
    "    sentences = make_sentences(text)\n",
    "    print(f\"{fname.name} contains {len(sentences)} sentences\")\n",
    "    write_to_file_cleaned(sentences, fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deven/projects/clean_plot/nbs\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "#| local\n",
    "%cd ~/projects/clean_plot/nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY was dead: to begin with. There is no doubt\n",
      "whatever about that. The register of his burial was\n",
      "signed by the clergyman, the clerk, the undertaker,\n",
      "and the chief mourner. Scrooge signed it: and\n",
      "Scrooge's name was good upon 'Change, for anything he\n",
      "chose to put his hand to. Old Marley was as dead as a\n",
      "door-nail.\n",
      "\n",
      "Mind! I don't mean to say that I know, of my\n",
      "own knowledge, what there is particularly dead about\n",
      "a door-nail. I might have been inclined, myself, to\n",
      "regard a coffin-nail as the deadest piece of ironmongery\n",
      "in the trade. But the wisdom of our ancestors\n",
      "is in the simile; and my unhallowed hands\n",
      "shall not disturb it, or the Country's done for. You\n",
      "will therefore permit me to repeat, emphatically, that\n",
      "Marley was as dead as a door-nail.\n",
      "\n",
      "This is a new sentence.\n"
     ]
    }
   ],
   "source": [
    "fname = '../files/dummy.txt'\n",
    "text = get_data(fname)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db9f58",
   "metadata": {},
   "source": [
    "It goes from this to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a1a40",
   "metadata": {},
   "source": [
    "The `clean` function writes these sentences into a txt file with the name `<fname>_cleaned.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_wordnet_pos(\n",
    "    word: str,  # input word token\n",
    ") -> str:  # POS of the given word\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_wordnet_pos(\n",
    "    word: str,  # input word token\n",
    ") -> str:  # POS of the given word\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_stopwords(sentence: str,  # input sentence\n",
    "                     ) -> str:    # output sentence\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes stopwords from it\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "    for word in sentence.split():\n",
    "        if word.lower() not in STOPWORDS:\n",
    "            sentences.append(word)\n",
    "    return \" \".join(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_punctuations(\n",
    "    sentence: str,  # input sentence\n",
    ") -> str:  # output sentence\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations from it\n",
    "    \"\"\"\n",
    "    pat2 = re.compile(r\"[^a-zA-Z0-9 ]+\")\n",
    "    pat1 = re.compile(r\"[\\s]+\")\n",
    "\n",
    "    doc = pat2.sub(\" \", sentence)\n",
    "    doc = pat1.sub(\" \", doc)\n",
    "    doc = doc.strip()\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_punc_clean(\n",
    "    sentence: str,  # input sentence\n",
    "    lemmatize: bool = False,  # flag to `lemmatize`\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations and stopwords from it\n",
    "\n",
    "    Will lemmatize words if `lemmatize = True`\n",
    "    \"\"\"\n",
    "    doc = remove_punctuations(sentence)\n",
    "    doc = remove_stopwords(doc)\n",
    "\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        doc = \" \".join(\n",
    "            [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc.split()]\n",
    "        )\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def process_for_lexical(fname: str) -> L:  # name of the input txt file  #\n",
    "    \"Given an input txt file, return removed sentences\"\n",
    "    fname = Path(fname)\n",
    "    all_data = get_data(fname)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print(\"Done processing\", fname.name)\n",
    "    return L(removed_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81e938",
   "metadata": {},
   "source": [
    "def process_for_lexical(fname: str) -> L:  # name of the input txt file  #\n",
    "    \"Given an input txt file, return removed sentences\"\n",
    "    fname = Path(fname)\n",
    "    all_data = get_data(fname)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print(\"Done processing\", fname.name)\n",
    "    return L(removed_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847a77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(fname)\n",
    "sentences = make_sentences(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ce9bf",
   "metadata": {},
   "source": [
    "Let's continue the same example from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7110f93",
   "metadata": {},
   "source": [
    "Here, the `remove_punc_clean` function removes punctuations, STOPWORDS and lemmatizes the word and returns the cleaned sentence. \n",
    "\n",
    ":::{.callout-Note}\n",
    "\n",
    "It is possible that a sentence may be removed completely as it may contain only STOPWORDS.\n",
    "\n",
    ":::\n",
    "This method is to be used for methods involving lexical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458cbc5a",
   "metadata": {},
   "source": [
    "Without lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e686b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial signed clergyman clerk undertaker chief mourner\n",
      "Scrooge signed Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestors simile unhallowed hands shall disturb Country done\n",
      "therefore permit repeat emphatically Marley dead door nail\n",
      "new sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1fa71",
   "metadata": {},
   "source": [
    "With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial sign clergyman clerk undertaker chief mourner\n",
      "Scrooge sign Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestor simile unhallowed hand shall disturb Country do\n",
      "therefore permit repeat emphatically Marley dead door nail\n",
      "new sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb273d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy.txt contains 11 sentences\n"
     ]
    }
   ],
   "source": [
    "clean('../files/dummy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b94240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing dummy.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_for_lexical('../files/dummy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdc5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('../files/dummy.txt'),Path('../files/dummy_cleaned.txt')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('../files/').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def num_words(sentence: str) -> int:  # input sentence  # number of words\n",
    "    \"Returns the number of words in a sentence\"\n",
    "    return len(remove_punctuations(sentence).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94876a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(sentence: str) -> int:  # input sentence  # number of words\n",
    "    \"Returns the number of words in a sentence\"\n",
    "    return len(remove_punctuations(sentence).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08901522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no doubt whatever about that.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences[1])\n",
    "num_words(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aea3b2",
   "metadata": {},
   "source": [
    "### Patches to `pathlib.Path`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdd7cf1",
   "metadata": {},
   "source": [
    "With all these utility functions, these are just some additional functions which are applied to `pathlib.Path`. There are 3 additional functions/properties when you have a numpy array or a txt file inside a Path object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6941ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch(as_prop=True)\n",
    "def shape(self: Path):\n",
    "    name = str(self)\n",
    "    if name.endswith(\".npy\"):\n",
    "        return np.load(self).shape\n",
    "    raise AssertionError(\"not a npy array\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c7cc0",
   "metadata": {},
   "source": [
    "@patch(as_prop=True)\n",
    "def shape(self: Path):\n",
    "    name = str(self)\n",
    "    if name.endswith(\".npy\"):\n",
    "        return np.load(self).shape\n",
    "    raise AssertionError(\"not a npy array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.utils import working_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063649ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|local\n",
    "with working_directory('/home/deven'):\n",
    "    p = 'test.npy'\n",
    "    arr = np.load(p)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1499dfe",
   "metadata": {},
   "source": [
    "Instead of all of that, I can just call `Path().shape`, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| local\n",
    "with working_directory('/home/deven'): \n",
    "    shp = Path('test.npy').shape\n",
    "    test_eq(arr.shape, Path('test.npy').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3d082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deven/projects/clean_plot/files\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "%cd ../files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch(as_prop=True)\n",
    "def text(self: Path):\n",
    "    if str(self).endswith(\".txt\"):\n",
    "        with open(self) as f:\n",
    "            return f.read()\n",
    "    raise AssertionError(\"not a txt file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24801585",
   "metadata": {},
   "source": [
    "@patch(as_prop=True)\n",
    "def text(self: Path):\n",
    "    if str(self).endswith(\".txt\"):\n",
    "        with open(self) as f:\n",
    "            return f.read()\n",
    "    raise AssertionError(\"not a txt file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100e586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MARLEY was dead: to begin with. There is no doubt\\nwhatever about that. The register of his burial was\\nsigned by the clergyman, the clerk, the undertaker,\\nand the chief mourner. Scrooge signed it: and\\nScrooge's name was good upon 'Change, for anything he\\nchose to put his hand to. Old Marley was as dead as a\\ndoor-nail.\\n\\nMind! I don't mean to say that I know, of my\\nown knowledge, what there is particularly dead about\\na door-nail. I might have been inclined, myself, to\\nregard a coffin-nail as the deadest piece of ironmongery\\nin the trade. But the wisdom of our ancestors\\nis in the simile; and my unhallowed hands\\nshall not disturb it, or the Country's done for. You\\nwill therefore permit me to repeat, emphatically, that\\nMarley was as dead as a door-nail.\\n\\nThis is a new sentence.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('dummy.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac82154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch(as_prop=True)\n",
    "def sentences(self: Path):\n",
    "    name = str(self)\n",
    "    if name.endswith(\".txt\"):\n",
    "        if name.endswith(\"_cleaned.txt\"):\n",
    "            return split_by_newline(self.text)\n",
    "        else:\n",
    "            return make_sentences(self.text)\n",
    "    raise AssertionError(\"not a txt file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def sentences(self: Path):\n",
    "    name = str(self)\n",
    "    if name.endswith(\".txt\"):\n",
    "        if name.endswith(\"_cleaned.txt\"):\n",
    "            return split_by_newline(self.text)\n",
    "        else:\n",
    "            return make_sentences(self.text)\n",
    "    raise AssertionError(\"not a txt file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
