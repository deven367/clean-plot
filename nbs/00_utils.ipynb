{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c03cfe",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Various utils for cleaning, organining and capturing other information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326490e4",
   "metadata": {},
   "source": [
    "## Generic utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda87a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from fastcore.foundation import L\n",
    "from fastcore.xtras import globtastic\n",
    "import pathlib\n",
    "from fastcore.test import test_eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_data(\n",
    "    fname: (str, Path) # path to the file\n",
    "    )->str: # returns content of the file\n",
    "    \"Reads from a txt file\"\n",
    "    with open(fname, 'r') as f:\n",
    "        all_text = f.read()\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73089b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_pmi(\n",
    "    fname: (str, Path)  # name of pmi file\n",
    ") -> np.ndarray:  # pmi matrix\n",
    "    \"\"\"\n",
    "    Loads the PMI matrix\n",
    "    \"\"\"\n",
    "    file_ = loader(fname, '.npy')\n",
    "    pmi = np.load(file_)\n",
    "    print(f'Loaded {name}')\n",
    "    return pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fba8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def loader(\n",
    "    path: [str, pathlib.Path],  # path to a given folder,\n",
    "    extension: str,  # extension of the file you want\n",
    ") -> L:  # returns `L`\n",
    "    \"Given a Path and an extension, returns all files with the extension in the path\"\n",
    "    files = L([Path(f) for f in globtastic(path, file_glob=f'*{extension}')])\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16658b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_dictionary(\n",
    "    fname: str , # path to the pkl file\n",
    "    )->dict: # returns the contents \n",
    "    \"\"\"\n",
    "    Given a fname, function loads a `pkl` dictionary\n",
    "    from the current directory\n",
    "    \"\"\"\n",
    "    fname = open(fname, 'rb')\n",
    "    data = pickle.load(fname)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b028e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def normalize(\n",
    "    data: np.ndarray,  # input array\n",
    ") -> np.ndarray:  # normalized array\n",
    "    \"\"\"\n",
    "    Given an input array, return normalized array\n",
    "    \"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd14c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(normalize([1, 2, 3, 4, 5]), [0.  , 0.25, 0.5 , 0.75, 1.  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcbf58",
   "metadata": {},
   "source": [
    "## Utils for cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1571a87",
   "metadata": {},
   "source": [
    "Before using any of the cleaning utils in the file, please run `download_nltk_dep` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f592c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def download_nltk_dep():\n",
    "    \"\"\"\n",
    "    Downloads the `nltk` dependencies\n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ad0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def split_by_newline(\n",
    "    text: str, # sentences separated by \\n\n",
    "    ) -> L: # list of sentences\n",
    "    \"\"\"\n",
    "    Only use when sentences are already tokenized\n",
    "    returns sentences split by '\\\\n' if len(line) > 0\n",
    "\n",
    "    Args:\n",
    "        all (str): tokenized string to be split by '\\\\n'\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences split by '\\\\n'\n",
    "    \"\"\"\n",
    "    return L([line for line in text.split('\\n') if len(line) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8a7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) ['Hello there!','This is how this functions works!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello there!\\nThis is how this functions works!\"\n",
    "split_by_newline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rm_useless_spaces(\n",
    "    t: str, # sentence with extra spaces\n",
    "    ) -> str: # sentence without extra spaces\n",
    "    \"\"\"\n",
    "    Removes useless spaces\n",
    "    \"\"\"\n",
    "    _re_space = re.compile(' {2,}')\n",
    "    return _re_space.sub(' ', t).lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdb50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is test sentence. This removes all the extra spaces.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_useless_spaces('  This is      test sentence.  This removes  all the extra  spaces.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff8004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def make_sentences(\n",
    "    text: str, # bulk text\n",
    "    ) -> L: # list of sentences\n",
    "    \"\"\"\n",
    "    Converts given bulk into sentences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sent_tokenize('')\n",
    "    except Exception as error:\n",
    "        download_nltk_dep()\n",
    "        print(f'Run download_nltk_dep() first') \n",
    "#     all_cleaned = re.sub('\\n', ' ', text)\n",
    "    all_cleaned = text.replace('\\n', ' ')\n",
    "    all_cleaned = rm_useless_spaces(all_cleaned)\n",
    "    all_cleaned = all_cleaned.strip()\n",
    "    all_cleaned = unidecode.unidecode(all_cleaned)\n",
    "    sentences = sent_tokenize(all_cleaned)\n",
    "    return L(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71078cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def write_to_file_cleaned(\n",
    "    sentences: list, # list of sentences \n",
    "    fname: str, # name of output file\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Writes the sentences to a .txt file\n",
    "    \"\"\"\n",
    "    with open(f'{fname.stem}_cleaned.txt', 'w') as f:\n",
    "        for line in sentences:\n",
    "            f.write(f'{line}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ddbd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@call_parse\n",
    "def clean(\n",
    "    fname: str, # name of input txt file\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Takes name of a txt file and writes the tokenized sentences into a new txt file\n",
    "    \"\"\"\n",
    "    fname = Path(fname)\n",
    "    text = get_data(fname)\n",
    "    sentences = make_sentences(text)\n",
    "    print(f'{fname.name} contains {len(sentences)} sentences')\n",
    "    write_to_file_cleaned(sentences, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abe056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ffb3b",
   "metadata": {},
   "source": [
    "All functions mentioned above are merged into a single function called `clean`. \n",
    "You only need to give it the name of the .txt file that you want to clean and call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7bb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY was dead: to begin with. There is no doubt\n",
      "whatever about that. The register of his burial was\n",
      "signed by the clergyman, the clerk, the undertaker,\n",
      "and the chief mourner. Scrooge signed it: and\n",
      "Scrooge's name was good upon 'Change, for anything he\n",
      "chose to put his hand to. Old Marley was as dead as a\n",
      "door-nail.\n",
      "\n",
      "Mind! I don't mean to say that I know, of my\n",
      "own knowledge, what there is particularly dead about\n",
      "a door-nail. I might have been inclined, myself, to\n",
      "regard a coffin-nail as the deadest piece of ironmongery\n",
      "in the trade. But the wisdom of our ancestors\n",
      "is in the simile; and my unhallowed hands\n",
      "shall not disturb it, or the Country's done for. You\n",
      "will therefore permit me to repeat, emphatically, that\n",
      "Marley was as dead as a door-nail.\n",
      "\n",
      "This is a new sentence.\n"
     ]
    }
   ],
   "source": [
    "fname = '../files/dummy.txt'\n",
    "text = get_data(fname)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db9f58",
   "metadata": {},
   "source": [
    "It goes from this to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff72363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05488f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentences(get_data(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11a1a40",
   "metadata": {},
   "source": [
    "The `clean` function writes these sentences into a txt file with the name `<fname>_cleaned.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_wordnet_pos(\n",
    "    word: str, # input word token\n",
    "    ) -> str: # POS of the given word\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_stopwords(\n",
    "    sentence: str, # input sentence\n",
    "    ) -> str: # output sentence\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes stopwords from it\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    for word in sentence.split():\n",
    "        if word.lower() not in STOPWORDS:\n",
    "            sentences.append(word)\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_punctuations(\n",
    "    sentence: str, # input sentence\n",
    "    ) -> str: # output sentence\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations from it\n",
    "    \"\"\"\n",
    "    pat2 = re.compile('[^a-zA-Z0-9 ]+')\n",
    "    pat1 = re.compile('[\\s]+')\n",
    "\n",
    "    doc = pat2.sub(' ', sentence)\n",
    "    doc = pat1.sub(' ', doc)\n",
    "    doc = doc.strip()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_punc_clean(\n",
    "    sentence: str, # input sentence\n",
    "    lemmatize: bool = False, # flag to `lemmatize`\n",
    "    ) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations and stopwords from it\n",
    "    \n",
    "    Will lemmatize words if `lemmatize = True`\n",
    "    \"\"\"\n",
    "    doc = remove_punctuations(sentence)\n",
    "    doc = remove_stopwords(doc)\n",
    "    \n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        doc = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc.split()])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b58049",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "\n",
    "It is possible that while using `remove_punc_clean`, a sentence might get eliminated completely as it only contained stopwords.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def process_for_lexical(\n",
    "    fname: str, # name of the input txt file\n",
    "    ) -> L: # \n",
    "    \"Given an input txt file, return removed sentences\"\n",
    "    fname = Path(fname)\n",
    "    all_data = get_data(fname)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print('Done processing', fname.name)\n",
    "    return L(removed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81e938",
   "metadata": {},
   "source": [
    "### Example contd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847a77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(fname)\n",
    "sentences = make_sentences(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5ce9bf",
   "metadata": {},
   "source": [
    "Let's continue the same example from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7110f93",
   "metadata": {},
   "source": [
    "Here, the `remove_punc_clean` function removes punctuations, STOPWORDS and lemmatizes the word and returns the cleaned sentence. \n",
    "\n",
    ":::{.callout-Note}\n",
    "\n",
    "It is possible that a sentence may be removed completely as it may contain only STOPWORDS.\n",
    "\n",
    ":::\n",
    "This method is to be used for methods involving lexical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458cbc5a",
   "metadata": {},
   "source": [
    "Without lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e686b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial signed clergyman clerk undertaker chief mourner\n",
      "Scrooge signed Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestors simile unhallowed hands shall disturb Country done\n",
      "therefore permit repeat emphatically Marley dead door nail\n",
      "new sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1fa71",
   "metadata": {},
   "source": [
    "With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial sign clergyman clerk undertaker chief mourner\n",
      "Scrooge sign Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestor simile unhallowed hand shall disturb Country do\n",
      "therefore permit repeat emphatically Marley dead door nail\n",
      "new sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb273d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy.txt contains 11 sentences\n"
     ]
    }
   ],
   "source": [
    "clean('../files/dummy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b94240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing dummy.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_for_lexical('../files/dummy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdc5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('../files/dummy.txt'),Path('../files/dummy_cleaned.txt')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('../files/').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def num_words(\n",
    "    sentence: str, # input sentence\n",
    "    )->int: # number of words\n",
    "    \"Returns the number of words in a sentence\"\n",
    "    return len(remove_punctuations(sentence).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94876a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY was dead: to begin with.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "num_words(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08901522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no doubt whatever about that.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentences[1])\n",
    "num_words(sentences[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
