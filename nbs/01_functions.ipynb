{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "> This includes majority of the functions for cleaning text files\n",
    "\n",
    "- hide_colab_badge: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from clean_plot.core import *\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from clean_plot.core import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "from fastcore.foundation import L\n",
    "from fastcore.test import test_eq, test_ne\n",
    "from fastcore.script import call_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from typing import Callable, Iterator, Union, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def download_ntlk_dep():\n",
    "    \"\"\"\n",
    "    Downloads the `nltk` dependencies\n",
    "    \"\"\"\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def normalize(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function takes an array, matrix as input and normalizes\n",
    "    it between 0 and 1\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): any 1-D, or 2-D numpy array\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): normalized ndarray\n",
    "    \"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(normalize([1,2,3,4,5]), [0.  , 0.25, 0.5 , 0.75, 1.  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def split_by_newline(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Only use when sentences are already tokenized\n",
    "    returns sentences split by '\\\\n' if len(line) > 0\n",
    "\n",
    "    Args:\n",
    "        all (str): tokenized string to be split by '\\\\n'\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences split by '\\\\n'\n",
    "    \"\"\"\n",
    "    return L([line for line in text.split('\\n') if len(line) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) ['Hello there!','This is how this functions works!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello there!\\nThis is how this functions works!\"\n",
    "split_by_newline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rm_useless_spaces(t: str) -> str: \n",
    "    \"\"\"\n",
    "    Removes useless spaces\n",
    "    \"\"\"\n",
    "    _re_space = re.compile(' {2,}')\n",
    "    return _re_space.sub(' ', t).lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is test sentence. This removes all the extra spaces.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_useless_spaces('  This is      test sentence.  This removes  all the extra  spaces.  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def make_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Converts given bulk into sentences\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sent_tokenize('')\n",
    "    except Exception as error:\n",
    "        download_ntlk_dep()\n",
    "        print(f'Run download_nltk_dep() first') \n",
    "#     all_cleaned = re.sub('\\n', ' ', text)\n",
    "    all_cleaned = text.replace('\\n', ' ')\n",
    "    all_cleaned = rm_useless_spaces(all_cleaned)\n",
    "    all_cleaned = all_cleaned.strip()\n",
    "    all_cleaned = unidecode.unidecode(all_cleaned)\n",
    "    sentences = sent_tokenize(all_cleaned)\n",
    "    return L(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def write_to_file_cleaned(sentences: List[str], fname: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes the sentences to a .txt file\n",
    "    \"\"\"\n",
    "    with open(f'{fname.stem}_cleaned.txt', 'w') as f:\n",
    "        for line in sentences:\n",
    "            f.write(f'{line}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@call_parse\n",
    "def clean(fname: str) -> None:\n",
    "    \"\"\"\n",
    "    Takes name of a txt file and writes the tokenized sentences into a new txt file\n",
    "    \"\"\"\n",
    "    fname = Path(fname)\n",
    "    text = get_data(fname)\n",
    "    sentences = make_sentences(text)\n",
    "    print(f'{fname.name} contains {len(sentences)} sentences')\n",
    "    write_to_file_cleaned(sentences, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions mentioned above are merged into a single function called `clean`. \n",
    "You only need to give it the name of the .txt file that you want to clean and call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY was dead: to begin with. There is no doubt\n",
      "whatever about that. The register of his burial was\n",
      "signed by the clergyman, the clerk, the undertaker,\n",
      "and the chief mourner. Scrooge signed it: and\n",
      "Scrooge's name was good upon 'Change, for anything he\n",
      "chose to put his hand to. Old Marley was as dead as a\n",
      "door-nail.\n",
      "\n",
      "Mind! I don't mean to say that I know, of my\n",
      "own knowledge, what there is particularly dead about\n",
      "a door-nail. I might have been inclined, myself, to\n",
      "regard a coffin-nail as the deadest piece of ironmongery\n",
      "in the trade. But the wisdom of our ancestors\n",
      "is in the simile; and my unhallowed hands\n",
      "shall not disturb it, or the Country's done for. You\n",
      "will therefore permit me to repeat, emphatically, that\n",
      "Marley was as dead as a door-nail.\n",
      "\n",
      "This is a new sentence.\n"
     ]
    }
   ],
   "source": [
    "fname = '../files/dummy.txt'\n",
    "text = get_data(fname)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It goes from this to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentences(get_data(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean` function writes these sentences into a txt file with the name `<fname>_cleaned.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_wordnet_pos(word: str) -> str:\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    try:\n",
    "        nltk.pos_tag('x')\n",
    "    except:\n",
    "        print(f'Run download_nltk_dep() first')\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_stopwords(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes stopwords from it\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stopwords.words('english')\n",
    "    except:\n",
    "        print(f'Run download_nltk_dep() first')\n",
    "    sentences = []\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    for word in sentence.split():\n",
    "        if word.lower() not in STOPWORDS:\n",
    "            sentences.append(word)\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_punctuations(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations from it\n",
    "    \"\"\"\n",
    "    pat2 = re.compile('[^a-zA-Z0-9 ]+')\n",
    "    pat1 = re.compile('[\\s]+')\n",
    "\n",
    "    doc = pat2.sub(' ', sentence)\n",
    "    doc = pat1.sub(' ', doc)\n",
    "    doc = doc.strip()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def remove_punc_clean(sentence: str, lemmatize: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations and stopwords from it\n",
    "    \n",
    "    Will lemmatize words if `lemmatize = True`\n",
    "    \"\"\"\n",
    "    doc = remove_punctuations(sentence)\n",
    "    doc = remove_stopwords(doc)\n",
    "    \n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        doc = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc.split()])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def process(fname: str) -> List[str]:\n",
    "    fname = Path(fname)\n",
    "    all_data = get_data(fname)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print('Done processing', fname.name)\n",
    "    return L(removed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the same example from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.'...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(fname)\n",
    "sentences = make_sentences(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `remove_punc_clean` function removes punctuations, STOPWORDS and lemmatizes the word and returns the cleaned sentence. \n",
    "\n",
    "> Note: It is possible that a sentence may be removed completely as it may contain only STOPWORDS. \n",
    "This method is to be used for methods involving lexical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial signed clergyman clerk undertaker chief mourner\n",
      "Scrooge signed Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestors simile unhallowed hands shall disturb Country done\n",
      "therefore permit repeat emphatically Marley dead door nail\n",
      "new sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial sign clergyman clerk undertaker chief mourner\n",
      "Scrooge sign Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestor simile unhallowed hand shall disturb Country do\n",
      "therefore permit repeat emphatically Marley dead door nail\n",
      "new sentence\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy.txt contains 11 sentences\n"
     ]
    }
   ],
   "source": [
    "clean('../files/dummy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing dummy.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(#0) []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process('../files/dummy.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('../files/dummy.txt'),Path('../files/dummy_cleaned.txt')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('../files/').ls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
