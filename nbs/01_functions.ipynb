{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "> This includes majority of the functions for cleaning text files\n",
    "\n",
    "- hide_colab_badge: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from clean_plot.core import *\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import unidecode\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fastcore.foundation import L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Callable, Iterator, Union, Optional, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def download_ntlk_dep():\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "download_ntlk_dep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    The function takes an array, matrix as input and normalizes\n",
    "    it between 0 and 1\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): any 1-D, or 2-D numpy array\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): normalized ndarray\n",
    "    \"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_by_newline(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Only use when sentences are already tokenized\n",
    "    returns sentences split by '\\\\n' if len(line) > 0\n",
    "\n",
    "    Args:\n",
    "        all (str): tokenized string to be split by '\\\\n'\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences split by '\\\\n'\n",
    "    \"\"\"\n",
    "    return L([line for line in text.split('\\n') if len(line) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) ['Hello there!','This is how this functions works!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello there!\\nThis is how this functions works!\"\n",
    "split_by_newline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rm_useless_spaces(t: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove multiple spaces\n",
    "    \"\"\"\n",
    "    _re_space = re.compile(' {2,}')\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is test sentence. This removes all the extra spaces.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_useless_spaces('This is      test sentence.  This removes  all the extra  spaces.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Converts given bulk into sentences\n",
    "    \"\"\"\n",
    "    all_cleaned = text.replace('\\n',' ')\n",
    "    all_cleaned = rm_useless_spaces(all_cleaned)\n",
    "    all_cleaned = all_cleaned.strip()\n",
    "    all_cleaned = unidecode.unidecode(all_cleaned)\n",
    "    sentences = sent_tokenize(all_cleaned)\n",
    "    return L(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_to_file_cleaned(sentences: List[str], fname: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes the sentences to a .txt file\n",
    "    \"\"\"\n",
    "    with open(fname[:-4]+'_cleaned.txt', 'w') as f:\n",
    "        for line in sentences:\n",
    "            f.write(f'{line}\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean(fname: str) -> None:\n",
    "    \"\"\"\n",
    "    Takes name of a txt file and writes the tokenized sentences into a new txt file\n",
    "    \"\"\"\n",
    "    text = get_data(fname)\n",
    "    sentences = make_sentences(all)\n",
    "    print(f'{fname[:-4].title()} contains {len(sentences)} sentences')\n",
    "    write_to_file_cleaned(sentences, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions mentioned above are merged into a single function called `clean`. \n",
    "You only need to give it the name of the .txt file that you want to clean and call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MARLEY was dead: to begin with. There is no doubt\\nwhatever about that. The register of his burial was\\nsigned by the clergyman, the clerk, the undertaker,\\nand the chief mourner. Scrooge signed it: and\\nScrooge's name was good upon 'Change, for anything he\\nchose to put his hand to. Old Marley was as dead as a\\ndoor-nail.\\n\\nMind! I don't mean to say that I know, of my\\nown knowledge, what there is particularly dead about\\na door-nail. I might have been inclined, myself, to\\nregard a coffin-nail as the deadest piece of ironmongery\\nin the trade. But the wisdom of our ancestors\\nis in the simile; and my unhallowed hands\\nshall not disturb it, or the Country's done for. You\\nwill therefore permit me to repeat, emphatically, that\\nMarley was as dead as a door-nail.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '../files/dummy.txt'\n",
    "text = get_data(fname)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It goes from this to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentences(get_data(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean` function writes these sentences into a txt file with the name `<fname>_cleaned.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets move on to further cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create a set of STOPWORDS which we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_wordnet_pos(word: str) -> str:\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_stopwords(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes stopwords from it\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for word in sentence.split():\n",
    "        if word.lower() not in STOPWORDS:\n",
    "            sentences.append(word)\n",
    "    return ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_punctuations(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations from it\n",
    "    \"\"\"\n",
    "    pat2 = re.compile('[^a-zA-Z0-9 ]+')\n",
    "    pat1 = re.compile('[\\s]+')\n",
    "\n",
    "    doc = pat2.sub(' ', sentence)\n",
    "    doc = pat1.sub(' ', doc)\n",
    "    doc = doc.strip()\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_punc_clean(sentence: str, lemmatize: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Takes a sentence and removes punctuations and stopwords from it\n",
    "    \n",
    "    Will lemmatize words if `lemmatize = True`\n",
    "    \"\"\"\n",
    "    doc = remove_punctuations(sentence)\n",
    "    doc = remove_stopwords(doc)\n",
    "    \n",
    "    \n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        doc = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc.split()])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def process(fname: str) -> List[str]:\n",
    "    \n",
    "    all_data = get_data(fname)\n",
    "    all_data = unidecode.unidecode(all_data)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print('Done processing', fname)\n",
    "    return L(removed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the same example from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) ['MARLEY was dead: to begin with.','There is no doubt whatever about that.','The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",'Old Marley was as dead as a door-nail.','Mind!',\"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(fname)\n",
    "sentences = make_sentences(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `remove_punc_clean` function removes punctuations, STOPWORDS and lemmatizes the word and returns the cleaned sentence. \n",
    "\n",
    "> Note: It is possible that a sentence may be removed completely as it may contain only STOPWORDS. \n",
    "This method is to be used for methods involving lexical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial signed clergyman clerk undertaker chief mourner\n",
      "Scrooge signed Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestors simile unhallowed hands shall disturb Country done\n",
      "therefore permit repeat emphatically Marley dead door nail\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARLEY dead begin\n",
      "doubt whatever\n",
      "register burial sign clergyman clerk undertaker chief mourner\n",
      "Scrooge sign Scrooge name good upon Change anything chose put hand\n",
      "Old Marley dead door nail\n",
      "Mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestor simile unhallowed hand shall disturb Country do\n",
      "therefore permit repeat emphatically Marley dead door nail\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
