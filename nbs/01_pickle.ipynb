{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to pickle\n",
    "\n",
    "> This module contains the functions to export all the embeddings to a time series format, group them together and export it as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** The module has 2 main functions:\n",
    "* `create_dic` which creates individual `.pkl` files (for each chapter of the book) based on breakpoints of chapters given by the use. (use when the dataset is very huge. Visualizing the entire heatmap does not give a lot of information. \n",
    "* `create_dic_whole_book` creates a single `.pkl` file for the entire book. To be used when the dataset is relatively small in size i.e. 2000 - 2500 sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def label(arg):\n",
    "    \"\"\"\n",
    "    Returns the full name of the model based on the abbreviation\n",
    "    \"\"\"\n",
    "    switcher = {\n",
    "        'dcltr_base': \"DeCLUTR Base\",\n",
    "        'dcltr_sm': \"DeCLUTR Small\",\n",
    "        'distil': \"DistilBERT\",\n",
    "        'if_FT': \"InferSent FastText\",\n",
    "        'if_glove': \"InferSent GloVe\",\n",
    "        'roberta': \"RoBERTa\",\n",
    "        'use': \"USE\",\n",
    "        'new_lex': 'Lexical Vectors',\n",
    "        'old_lex': 'Lexical Weights',\n",
    "        'lexical_wt': 'Lexical Weights',\n",
    "        'lexical_wt_ssm': 'Lexical Weights',\n",
    "        'lex_vect': 'Lexical Vectors',\n",
    "        'lex_vect_corr_ts': 'Lexical Vectors (Corr)',\n",
    "        'mpnet': 'MPNet',\n",
    "        'minilm': 'MiniLM',\n",
    "        'xlm': 'XLM'\n",
    "    }\n",
    "    return switcher.get(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"\n",
    "    Returns the cosine similarity between 2 vectors. \n",
    "    \"\"\"\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def create_dict_whole_book(embedding_path, k):\n",
    "    mdict = {}\n",
    "    parent_dir = os.path.basename(os.path.dirname(embedding_path))\n",
    "    sub_dict = {}\n",
    "    for fx in os.listdir(embedding_path):\n",
    "        if fx.endswith('.npy'):\n",
    "            name = fx[:-4]\n",
    "            embed = np.load(embedding_path+fx)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            ts = successive_similarities(embed, k)\n",
    "\n",
    "            name = create_label_whole_book(method, parent_dir)\n",
    "\n",
    "            sub_dict[name] = ts\n",
    "\n",
    "        if fx.endswith('_vect.npy'):\n",
    "            name = fx[:-4]\n",
    "            embed = np.load(embedding_path+fx)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            # ts = successive_similarities(embed, k)\n",
    "\n",
    "            name = create_label_whole_book(method, parent_dir)\n",
    "\n",
    "            sub_dict[name] = embed\n",
    "\n",
    "\n",
    "        if fx.endswith('_wt.npy'):\n",
    "            name = fx[:-4]\n",
    "            embed = np.load(embedding_path+fx)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            # ts = successive_similarities(embed, k)\n",
    "\n",
    "            name = create_label_whole_book(method, parent_dir)\n",
    "\n",
    "            sub_dict[name] = embed\n",
    "\n",
    "        if fx.endswith('_corr_ts.npy'):\n",
    "            name = fx[:-4]\n",
    "            embed = np.load(embedding_path+fx)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            # ts = successive_similarities(embed, k)\n",
    "\n",
    "            name = create_label_whole_book(method, parent_dir)\n",
    "            print('Found Lex Corr', name)\n",
    "            sub_dict[name] = embed\n",
    "\n",
    "\n",
    "    mdict[0] = sub_dict\n",
    "    pickle.dump(mdict, open(parent_dir +'_whole.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def create_label_whole_book(method, parent_dir):\n",
    "    # returns only the method name\n",
    "    return label(method)\n",
    "\n",
    "    # Format of Book name + Method\n",
    "    # return parent_dir.title() + ' ' + label(method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def create_label(index, method, parent_dir):\n",
    "    met = label(method)\n",
    "    return 'Book ' +str(index + 1) + \" \" + parent_dir.title() + \" \" + met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_embed_method_and_name(\n",
    "    fname, # name of the file\n",
    "    )->(str, str): # name of file, embeddding method\n",
    "    \"\"\"\n",
    "    Returns the name of the file and the method by \n",
    "    splitting on the word '_cleaned_'\n",
    "    \"\"\"\n",
    "    t = fname.split('_cleaned_')\n",
    "    return  t[0].split()[-1], t[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
