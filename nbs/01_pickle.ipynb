{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting to pickle\n",
    "\n",
    "> This module contains the functions to export all the embeddings to a time series format, group them together and export it as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from fastcore.xtras import *\n",
    "from fastcore.script import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** The module has 2 main functions:\n",
    "* `create_dic` which creates individual `.pkl` files (for each chapter of the book) based on breakpoints of chapters given by the use. (use when the dataset is very huge. Visualizing the entire heatmap does not give a lot of information. \n",
    "* `create_dic_whole_book` creates a single `.pkl` file for the entire book. To be used when the dataset is relatively small in size i.e. 2000 - 2500 sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def label(\n",
    "    method:str # name of the method\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Returns the full name of the model based on the abbreviation\n",
    "    \"\"\"\n",
    "    switcher = {\n",
    "        'dcltr_base': \"DeCLUTR Base\",\n",
    "        'dcltr_sm': \"DeCLUTR Small\",\n",
    "        'distil': \"DistilBERT\",\n",
    "        'if_FT': \"InferSent FastText\",\n",
    "        'if_glove': \"InferSent GloVe\",\n",
    "        'roberta': \"RoBERTa\",\n",
    "        'use': \"USE\",\n",
    "        'new_lex': 'Lexical Vectors',\n",
    "        'old_lex': 'Lexical Weights',\n",
    "        'lexical_wt': 'Lexical Weights',\n",
    "        'lexical_wt_ssm': 'Lexical Weights',\n",
    "        'lex_vect': 'Lexical Vectors',\n",
    "        'lex_vect_corr_ts': 'Lexical Vectors (Corr)',\n",
    "        'mpnet': 'MPNet',\n",
    "        'minilm': 'MiniLM',\n",
    "        'xlm': 'XLM'\n",
    "    }\n",
    "    return switcher.get(method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def cos_sim(\n",
    "    a: np.ndarray, # vector 1 \n",
    "    b: np.ndarray, # vector 2\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Returns the cosine similarity between 2 vectors. \n",
    "    \"\"\"\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def successive_similarities(embeddings, k):\n",
    "    successive = []\n",
    "    for i in range(len(embeddings) - k):\n",
    "        successive.append(cos_sim(embeddings[i], embeddings[i+k]))\n",
    "    return successive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "@call_parse\n",
    "def create_dict_whole_book(\n",
    "    embedding_path:str = '.', # path to the embeddings\n",
    "    k:int=1, # consecutive index\n",
    "    ):\n",
    "    \"Create pkl for time series from embeddings\"\n",
    "    p = Path(embedding_path).absolute()\n",
    "    book_name = p.stem.replace('_', ' ').title()\n",
    "    \n",
    "    mdict = {}\n",
    "    parent_dir = os.path.basename(os.path.dirname(embedding_path))\n",
    "    sub_dict = {}\n",
    "    \n",
    "    files = globtastic(p, recursive=False, file_glob='*.npy').map(Path)\n",
    "    flen = files.__len__()\n",
    "    if flen < 1:\n",
    "        print(f'Found {flen} embeddings')\n",
    "        print(f'Check `embedding path` and try again')\n",
    "        return\n",
    "        \n",
    "    print(f'Book Name: {book_name}')\n",
    "    print(f'Found {flen} methods')\n",
    "    print('-'*45)\n",
    "    \n",
    "    for f in files:\n",
    "        name = f.stem\n",
    "\n",
    "        if name.endswith('_vect'):\n",
    "            embed = np.load(f)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            # ts = successive_similarities(embed, k)\n",
    "            sub_dict[name] = embed\n",
    "\n",
    "\n",
    "        elif name.endswith('_wt'):\n",
    "            embed = np.load(f)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            # ts = successive_similarities(embed, k)\n",
    "            sub_dict[name] = embed\n",
    "\n",
    "        elif name.endswith('_corr_ts'):\n",
    "            embed = np.load(f)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            # ts = successive_similarities(embed, k)\n",
    "            print('Found Lex Corr', name)\n",
    "            sub_dict[name] = embed\n",
    "        \n",
    "        else:\n",
    "            embed = np.load(f)\n",
    "            book_name, method = get_embed_method_and_name(name)\n",
    "            \n",
    "            ts = successive_similarities(embed, k)\n",
    "            name = create_label_whole_book(method, parent_dir)\n",
    "            print(f'Found {name}')\n",
    "            sub_dict[name] = ts\n",
    "\n",
    "    mdict[0] = sub_dict\n",
    "    \n",
    "    new_path = p/'pkl'\n",
    "    new_path.mkdir(exist_ok = True)\n",
    "    pickle.dump(mdict, open(new_path/f'{book_name}_whole.pkl', 'wb'))\n",
    "    print('-'*45)\n",
    "    print(f'Saved pkl at {new_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def create_label_whole_book(method, parent_dir):\n",
    "    # returns only the method name\n",
    "    return label(method)\n",
    "\n",
    "    # Format of Book name + Method\n",
    "    # return parent_dir.title() + ' ' + label(method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def create_label(index, method, parent_dir):\n",
    "    met = label(method)\n",
    "    return 'Book ' +str(index + 1) + \" \" + parent_dir.title() + \" \" + met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_embed_method_and_name(\n",
    "    fname, # name of the file\n",
    "    )->(str, str): # name of file, embeddding method\n",
    "    \"\"\"\n",
    "    Returns the name of the file and the method by \n",
    "    splitting on the word '_cleaned_'\n",
    "    \"\"\"\n",
    "    t = fname.split('_cleaned_')\n",
    "    return  t[0].split()[-1], t[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/deven\n"
     ]
    }
   ],
   "source": [
    "#| local\n",
    "%cd ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Name: A Modest Proposal\n",
      "Found 10 methods\n",
      "---------------------------------------------\n",
      "Found DeCLUTR Small\n",
      "Found RoBERTa\n",
      "Found InferSent GloVe\n",
      "Found InferSent FastText\n",
      "Found DistilBERT\n",
      "Found XLM\n",
      "Found MPNet\n",
      "Found USE\n",
      "Found DeCLUTR Base\n",
      "Found MiniLM\n",
      "---------------------------------------------\n",
      "Saved pkl at /home/deven/embeddings/A_Modest_Proposal/pkl\n"
     ]
    }
   ],
   "source": [
    "#| local\n",
    "create_dict_whole_book('embeddings/A_Modest_Proposal', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
