{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99648f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#| nbflags skip_exec\n",
    "import re\n",
    "from clean_plot import *\n",
    "import os\n",
    "import unidecode\n",
    "from collections import OrderedDict\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d45b0",
   "metadata": {},
   "source": [
    "# Lexical\n",
    "\n",
    "> This file goes over generating lexical embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d5801",
   "metadata": {},
   "source": [
    ":::{.callout-Note}\n",
    "\n",
    "This module will be rewritten. Use at your own risk!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92362780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def interpolate(lex, removed_indices = []):\n",
    "    \"\"\"\n",
    "    Method does interpolation based on the removed indices.\n",
    "    Substitutes the missing values based on the previous value in the array\n",
    "    \"\"\"\n",
    "    for index in removed_indices:\n",
    "        if index < len(lex):\n",
    "            lex = np.insert(lex, index, lex[index - 1])\n",
    "    return lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b4ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_pmi(path):\n",
    "    pmi = np.load(path)\n",
    "    return pmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef07dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_dictionary(path):\n",
    "    fname = open(path, 'rb')\n",
    "    data = pickle.load(fname)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def write_to_file_lexical(sentences, fname):\n",
    "    with open(fname[:-4]+'_lexical.txt', 'w') as f:\n",
    "        for line in sentences:\n",
    "            f.write(line + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e49d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def process_v2(fname):\n",
    "    all_data = get_data(fname)\n",
    "    all_data = unidecode.unidecode(all_data)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    write_to_file_lexical(clean_sentences, fname)\n",
    "    print('Done processing', fname)\n",
    "    return removed_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
