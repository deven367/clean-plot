{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "> This includes majority of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from clean_plot import * \n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Deven\n",
      "[nltk_data]     Mistry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Deven\n",
      "[nltk_data]     Mistry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Deven Mistry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Deven\n",
      "[nltk_data]     Mistry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "#hide\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import unidecode\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    The function takes an array, matrix as input and normalizes\n",
    "    it between 0 and 1\n",
    "\n",
    "    Args:\n",
    "        data (ndarray): any 1-D, or 2-D numpy array\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): normalized ndarray\n",
    "    \"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_by_newline(all):\n",
    "    \"\"\"\n",
    "    Only use when sentences are already tokenized\n",
    "    returns sentences split by '\\\\n' if len(line) > 0\n",
    "\n",
    "    Args:\n",
    "        all (str): tokenized string to be split by '\\\\n'\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentences split by '\\\\n'\n",
    "    \"\"\"\n",
    "    return [line.lower() for line in all.split('\\n') if len(line) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello there!', 'this is how this functions works!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all = \"Hello there!\\nThis is how this functions works!\"\n",
    "split_by_newline(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    _re_space = re.compile(' {2,}')\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is test sentence. This removes all the extra spaces .'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_useless_spaces('This is      test sentence.  This removes  all the extra  spaces .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_sentences(all):\n",
    "    all_cleaned = all.replace('\\n',' ')\n",
    "    all_cleaned = rm_useless_spaces(all_cleaned)\n",
    "    all_cleaned = all_cleaned.strip()\n",
    "    all_cleaned = unidecode.unidecode(all_cleaned)\n",
    "    sentences = sent_tokenize(all_cleaned)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def write_to_file_cleaned(sentences, fname):\n",
    "    with open(fname[:-4]+'_cleaned.txt', 'w') as f:\n",
    "        for line in sentences:\n",
    "            f.write(line + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean(fname):\n",
    "    all = get_data(fname)\n",
    "    sentences = make_sentences(all)\n",
    "    print(fname[:-4].title() + ' contains {} sentences'.format(len(sentences)))\n",
    "    write_to_file_cleaned(sentences, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions mentioned above are merged into a single function called clean. \n",
    "You only need to give it the name of the .txt file that you want to clean and call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MARLEY was dead: to begin with. There is no doubt\\nwhatever about that. The register of his burial was\\nsigned by the clergyman, the clerk, the undertaker,\\nand the chief mourner. Scrooge signed it: and\\nScrooge's name was good upon 'Change, for anything he\\nchose to put his hand to. Old Marley was as dead as a\\ndoor-nail.\\n\\nMind! I don't mean to say that I know, of my\\nown knowledge, what there is particularly dead about\\na door-nail. I might have been inclined, myself, to\\nregard a coffin-nail as the deadest piece of ironmongery\\nin the trade. But the wisdom of our ancestors\\nis in the simile; and my unhallowed hands\\nshall not disturb it, or the Country's done for. You\\nwill therefore permit me to repeat, emphatically, that\\nMarley was as dead as a door-nail.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'dummy.txt'\n",
    "get_data(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It goes from this to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARLEY was dead: to begin with.',\n",
       " 'There is no doubt whatever about that.',\n",
       " 'The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\n",
       " \"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",\n",
       " 'Old Marley was as dead as a door-nail.',\n",
       " 'Mind!',\n",
       " \"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",\n",
       " 'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\n",
       " \"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",\n",
       " 'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_sentences(get_data(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean` function writes these sentences into a txt file with the name `<fname>_cleaned.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's move on to further cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first create a set of STOPWORDS which we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_stopwords(text):\n",
    "    sentence = []\n",
    "    for word in text.split():\n",
    "        if word not in STOPWORDS:\n",
    "            sentence.append(word)\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_punc_clean(sentence):\n",
    "    pat2 = re.compile('[^a-zA-Z0-9 ]+')\n",
    "    pat1 = re.compile('[\\s]+')\n",
    "\n",
    "    doc = pat2.sub(' ', sentence)\n",
    "    doc = pat1.sub(' ', doc)\n",
    "    doc = doc.strip().lower()\n",
    "\n",
    "    doc = remove_stopwords(doc)\n",
    "    # doc = ' '.join(list(OrderedDict.fromkeys(doc.split())))\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    doc = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in doc.split()])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(fname):\n",
    "    all_data = get_data(fname)\n",
    "    all_data = unidecode.unidecode(all_data)\n",
    "    sentences = make_sentences(all_data)\n",
    "    clean_sentences = []\n",
    "    removed_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = remove_punc_clean(sentence)\n",
    "        if len(t) > 0:\n",
    "            clean_sentences.append(t)\n",
    "        else:\n",
    "            removed_sentences.append(i)\n",
    "\n",
    "    # write_to_file_lexical(clean_sentences, fname)\n",
    "    print('Done processing', fname)\n",
    "    return removed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue the same example from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MARLEY was dead: to begin with.',\n",
       " 'There is no doubt whatever about that.',\n",
       " 'The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner.',\n",
       " \"Scrooge signed it: and Scrooge's name was good upon 'Change, for anything he chose to put his hand to.\",\n",
       " 'Old Marley was as dead as a door-nail.',\n",
       " 'Mind!',\n",
       " \"I don't mean to say that I know, of my own knowledge, what there is particularly dead about a door-nail.\",\n",
       " 'I might have been inclined, myself, to regard a coffin-nail as the deadest piece of ironmongery in the trade.',\n",
       " \"But the wisdom of our ancestors is in the simile; and my unhallowed hands shall not disturb it, or the Country's done for.\",\n",
       " 'You will therefore permit me to repeat, emphatically, that Marley was as dead as a door-nail.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(fname)\n",
    "sentences = make_sentences(data)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `remove_punc_clean` function removes punctuations, STOPWORDS and lemmatizes the word and returns the cleaned sentence. \n",
    "\n",
    "**NOTE** It is possible that a sentence gets removed completely as it may contain only STOPWORDS. \n",
    "This method is to be used for methods involving lexical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marley dead begin\n",
      "doubt whatever\n",
      "register burial sign clergyman clerk undertaker chief mourner\n",
      "scrooge sign scrooge name good upon change anything chose put hand\n",
      "old marley dead door nail\n",
      "mind\n",
      "mean say know knowledge particularly dead door nail\n",
      "might inclined regard coffin nail deadest piece ironmongery trade\n",
      "wisdom ancestor simile unhallowed hand shall disturb country do\n",
      "therefore permit repeat emphatically marley dead door nail\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print(remove_punc_clean(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
